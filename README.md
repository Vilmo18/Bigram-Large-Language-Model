# Bigram-Large-model
This projet is an ambitious project just to understand some concept in NLP (just for the fun)

![Git Badge](https://img.shields.io/badge/-Git-blue?style=flat&logo=Git&logoColor=white)
[![Python Badge](https://img.shields.io/badge/-Python-blue?style=flat&logo=Python&logoColor=white)](https://www.python.org)
![NumPy Badge](https://img.shields.io/badge/-NumPy-blue?style=flat&logo=NumPy&logoColor=white)
![PyTorch Badge](https://img.shields.io/badge/-PyTorch-blue?style=flat&logo=PyTorch&logoColor=white)
![GCP Badge](https://img.shields.io/badge/-GCP-blue?style=flat&logo=googlecloud&logoColor=white)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1FXRLuExHk9YJD1QhShWWO1qM-LYmWJhq?usp=sharing)


### Visualisation of the dataset

To better understand our training dataset, we use t-SNE for dimensionality reduction and K-Means for clustering.

<p align="center">
  <img src="images/attention_research.jpg" alt="train" width="200"/>
</p>
